# ğŸš€ Multi-Modal Physical Exercise Classification Project

## ğŸ“Œ Project Overview
This project focuses on classifying physical exercises using multi-modal data (accelerometer and depth camera) from the **MEx dataset** ([UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/MEx#)). The goal is to develop user-independent models to recognize 7 types of exercises performed by subjects lying on a mat. The project explores data fusion techniques at different levels (unimodal, feature-level, decision-level) and includes a bonus task for biometric identification.

### âœ¨ Key Features
- **ğŸ“Š Data Preprocessing**: Windowing of raw data into 5-second sequences with 3-second overlap.
- **ğŸ”€ Multi-Modal Fusion**: Techniques to combine accelerometer and depth camera data.
- **ğŸ¤– Classification**: Evaluation using confusion matrices and F1 scores.
- **ğŸ“· Visualization**: Examples of accelerometer time-series and depth camera images.
- **ğŸ†” Biometric Identification**: Optional task to identify subjects from the dataset.

---

## ğŸ“‚ Dataset
A subset of the **MEx dataset** is used, containing:
- **ğŸ“¡ Accelerometer Data**: 500x3 matrix (3-axis measurements from a thigh sensor).
- **ğŸ“¸ Depth Camera Data**: 5x192 matrix (aerial view frames).

The dataset is preprocessed into shorter sequences to generate training/testing examples.  
ğŸ“ **Dataset Location**: Provided as a subset in the project files (not included in this repository).

---

## ğŸ“‘ Project Structure
The workflow is divided into phases:

### 1ï¸âƒ£ **Data Preparation, Exploration, and Visualization**
  - ğŸ“¥ Loading raw data from CSV files.
  - âœ‚ï¸ Windowing long sequences into 5-second segments.
  - ğŸ“Š Visualization of accelerometer time-series and depth images.

### 2ï¸âƒ£ **Unimodal Fusion for Classification**
  - ğŸ·ï¸ Feature extraction for accelerometer and depth camera data separately.
  - ğŸ† Classification using models like KNN, LDA, or PCA.

### 3ï¸âƒ£ **Feature-Level Fusion for Multimodal Classification**
  - ğŸ”— Combining features from both modalities before classification.

### 4ï¸âƒ£ **Decision-Level Fusion for Multimodal Classification**
  - ğŸ—ï¸ Aggregating predictions from unimodal models.

### 5ï¸âƒ£ **Bonus Task: Multimodal Biometric Identification**
  - ğŸ†” Identifying individuals using sensor data.

---

## ğŸ”¬ Methods & Techniques Used
### ğŸ› ï¸ Data Preprocessing
- ğŸ“ **Feature scaling** using `StandardScaler` and `MinMaxScaler`.
- ğŸ”  **Encoding categorical variables** using `LabelEncoder`.
- ğŸ“‰ **Dimensionality reduction** via `PCA` and `LDA`.

### ğŸ§  Machine Learning Models Implemented
- ğŸ¤– **Support Vector Machines (SVM)** (`SVC`)
- ğŸ” **K-Nearest Neighbors (KNN)** (`KNeighborsClassifier`)
- ğŸ§® **NaÃ¯ve Bayes Classifier** (`GaussianNB`)
- âš¡ **Adaptive Boosting (AdaBoost)** (`AdaBoostClassifier`)
- ğŸ¯ **Calibrated Classifier** (`CalibratedClassifierCV`)

### ğŸ“ˆ Model Evaluation
- ğŸ“Š **Confusion Matrix** (`confusion_matrix` from `sklearn`)
- ğŸ” **Grid Search for Hyperparameter Tuning** (`GridSearchCV`)

## ğŸ”— Dependencies
Ensure you have the following Python libraries installed before running the notebook:

```bash
pip install numpy pandas matplotlib scipy scikit-learn pillow
```

## ğŸƒ Usage
Clone the repository and navigate to the project folder:

```bash
git clone https://github.com/Abu-Taher-web/MULTI-MODAL-DATA-FUSION.git
cd MULTI-MODAL-DATA-FUSION
```

Run the Jupyter Notebook:

```bash
jupyter notebook Final_Group_project_work_MEx_classification_Submission.ipynb
```

## ğŸ¯ Results
- âœ… The project successfully applies **multi-modal data fusion** techniques to classify physical exercises.
- ğŸ“Š Various **machine learning models** are compared, and the best-performing model is identified.
- ğŸ† Feature engineering and **dimensionality reduction** improve classification accuracy.

## ğŸ‘¨â€ğŸ’» Contributors
- **ğŸ“ Abu Taher**
- **ğŸ“ Md. Rabiul Hasan**

## ğŸ“œ License
This project is released under the **MIT License**.

---

